{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f435cde-d6d1-41af-b9cf-e46c5628510e",
   "metadata": {},
   "source": [
    "# Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).     \n",
    "- Convert text to lowercase and remove punctuation.  \n",
    "- Tokenize the text into words and sentences.  \n",
    "- Remove stopwords (using NLTK's stopwords list).\n",
    "- Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f627b694-4ce7-49b9-9fed-5058d85db82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\A/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\A/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\A/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\A/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.stem     import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6957eee8-6e90-4f31-ba95-16be4ae4ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My fascination lies deeply within the realm of quantum mechanics, a universe governed by rules that often defy our everyday intuition. The concept of superposition, where a particle can exist in multiple states simultaneously until observed, constantly sparks my curiosity about the fundamental nature of reality. Entanglement, the seemingly spooky connection between particles that allows them to instantaneously influence each other regardless of distance, hints at a deeper interconnectedness we are only beginning to grasp. Exploring the mathematical elegance that underpins these phenomena, from the Schrödinger equation to quantum field theory, feels like deciphering the very language of the cosmos. While still largely theoretical in many aspects, the potential applications of quantum computing and quantum cryptography promise to revolutionize technology as we know it, making this field an endlessly captivating frontier of scientific inquiry.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "new_paragraph = \"\"\"My fascination lies deeply within the realm of quantum mechanics, a universe governed by rules that often defy our everyday intuition. The concept of superposition, where a particle can exist in multiple states simultaneously until observed, constantly sparks my curiosity about the fundamental nature of reality. Entanglement, the seemingly spooky connection between particles that allows them to instantaneously influence each other regardless of distance, hints at a deeper interconnectedness we are only beginning to grasp. Exploring the mathematical elegance that underpins these phenomena, from the Schrödinger equation to quantum field theory, feels like deciphering the very language of the cosmos. While still largely theoretical in many aspects, the potential applications of quantum computing and quantum cryptography promise to revolutionize technology as we know it, making this field an endlessly captivating frontier of scientific inquiry.\"\"\"\n",
    "new_lines = new_paragraph.split(\"\\n\")\n",
    "print(new_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60e71e9-0f64-407f-99f7-3c7142900ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['fascination', 'lies', 'deeply', 'within', 'realm', 'quantum', 'mechanics', 'universe', 'governed', 'rules', 'often', 'defy', 'everyday', 'intuition', 'concept', 'superposition', 'particle', 'exist', 'multiple', 'states', 'simultaneously', 'observed', 'constantly', 'sparks', 'curiosity', 'fundamental', 'nature', 'reality', 'entanglement', 'seemingly', 'spooky', 'connection', 'particles', 'allows', 'instantaneously', 'influence', 'regardless', 'distance', 'hints', 'deeper', 'interconnectedness', 'beginning', 'grasp', 'exploring', 'mathematical', 'elegance', 'underpins', 'phenomena', 'schrödinger', 'equation', 'quantum', 'field', 'theory', 'feels', 'like', 'deciphering', 'language', 'cosmos', 'still', 'largely', 'theoretical', 'many', 'aspects', 'potential', 'applications', 'quantum', 'computing', 'quantum', 'cryptography', 'promise', 'revolutionize', 'technology', 'know', 'making', 'field', 'endlessly', 'captivating', 'frontier', 'scientific', 'inquiry']\n",
      "Word Frequencies:\n",
      "FreqDist({'quantum': 4, 'field': 2, 'fascination': 1, 'lies': 1, 'deeply': 1, 'within': 1, 'realm': 1, 'mechanics': 1, 'universe': 1, 'governed': 1, ...})\n"
     ]
    }
   ],
   "source": [
    "# Process text: lowercase and punctuation removal\n",
    "processed_text = new_paragraph.lower()\n",
    "processed_text = processed_text.translate(str.maketrans('', '', string.punctuation))\n",
    "# .maketrans() creates a translation table mapping characters to replacements or None for deletion, used with .translate().\n",
    "\n",
    "# Break down into individual words and sentences\n",
    "individual_tokens = word_tokenize(processed_text)\n",
    "sentence_units = sent_tokenize(processed_text)\n",
    "\n",
    "# Filter out common, insignificant words\n",
    "common_words = set(stopwords.words(\"english\"))\n",
    "meaningful_words = [item for item in individual_tokens if item not in common_words]\n",
    "\n",
    "# Calculate and display word occurrences\n",
    "frequency_distribution = FreqDist(meaningful_words)\n",
    "print(\"Filtered Words:\", meaningful_words)\n",
    "print(\"Word Frequencies:\")\n",
    "frequency_distribution.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e8992-0c69-4c04-97ee-027c4aa8a46b",
   "metadata": {},
   "source": [
    "# Q2. Stemming and Lemmatization:  \n",
    "- Take the tokenized words from Question 1 (after stopword removal).  \n",
    "- Apply stemming using NLTK's PorterStemmer and LancasterStemmer.   \n",
    "- Apply lemmatization using NLTK's WordNetLemmatizer.   \n",
    "- Compare and display results of both techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3016d768-d1c8-489c-a4bd-28941dda4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['fascin', 'lie', 'deepli', 'within', 'realm', 'quantum', 'mechan', 'univers', 'govern', 'rule', 'often', 'defi', 'everyday', 'intuit', 'concept', 'superposit', 'particl', 'exist', 'multipl', 'state', 'simultan', 'observ', 'constantli', 'spark', 'curios', 'fundament', 'natur', 'realiti', 'entangl', 'seemingli', 'spooki', 'connect', 'particl', 'allow', 'instantan', 'influenc', 'regardless', 'distanc', 'hint', 'deeper', 'interconnected', 'begin', 'grasp', 'explor', 'mathemat', 'eleg', 'underpin', 'phenomena', 'schrödinger', 'equat', 'quantum', 'field', 'theori', 'feel', 'like', 'deciph', 'languag', 'cosmo', 'still', 'larg', 'theoret', 'mani', 'aspect', 'potenti', 'applic', 'quantum', 'comput', 'quantum', 'cryptographi', 'promis', 'revolution', 'technolog', 'know', 'make', 'field', 'endlessli', 'captiv', 'frontier', 'scientif', 'inquiri']\n",
      "\n",
      "Lancaster Stemmer: ['fascin', 'lie', 'deeply', 'within', 'realm', 'quant', 'mech', 'univers', 'govern', 'rul', 'oft', 'defy', 'everyday', 'intuit', 'conceiv', 'superposit', 'partic', 'ex', 'multipl', 'stat', 'simult', 'observ', 'const', 'spark', 'curios', 'funda', 'nat', 'real', 'entangl', 'seem', 'spooky', 'connect', 'partic', 'allow', 'inst', 'influ', 'regardless', 'dist', 'hint', 'deep', 'interconnect', 'begin', 'grasp', 'expl', 'mathem', 'eleg', 'underpin', 'phenomen', 'schrödinger', 'equ', 'quant', 'field', 'the', 'feel', 'lik', 'deciph', 'langu', 'cosmo', 'stil', 'larg', 'theoret', 'many', 'aspect', 'pot', 'apply', 'quant', 'comput', 'quant', 'cryptograph', 'prom', 'revolv', 'technolog', 'know', 'mak', 'field', 'endless', 'capt', 'fronty', 'sci', 'inquiry']\n",
      "\n",
      "Lemmatized: ['fascination', 'lie', 'deeply', 'within', 'realm', 'quantum', 'mechanic', 'universe', 'governed', 'rule', 'often', 'defy', 'everyday', 'intuition', 'concept', 'superposition', 'particle', 'exist', 'multiple', 'state', 'simultaneously', 'observed', 'constantly', 'spark', 'curiosity', 'fundamental', 'nature', 'reality', 'entanglement', 'seemingly', 'spooky', 'connection', 'particle', 'allows', 'instantaneously', 'influence', 'regardless', 'distance', 'hint', 'deeper', 'interconnectedness', 'beginning', 'grasp', 'exploring', 'mathematical', 'elegance', 'underpins', 'phenomenon', 'schrödinger', 'equation', 'quantum', 'field', 'theory', 'feel', 'like', 'deciphering', 'language', 'cosmos', 'still', 'largely', 'theoretical', 'many', 'aspect', 'potential', 'application', 'quantum', 'computing', 'quantum', 'cryptography', 'promise', 'revolutionize', 'technology', 'know', 'making', 'field', 'endlessly', 'captivating', 'frontier', 'scientific', 'inquiry']\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "port_stemmer = PorterStemmer()\n",
    "lanc_stemmer = LancasterStemmer()\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Use filtered_words from Q1\n",
    "port_result = [port_stemmer.stem(item) for item in meaningful_words]\n",
    "lanc_result = [lanc_stemmer.stem(item) for item in meaningful_words]\n",
    "lemma_output = [word_net_lemmatizer.lemmatize(item) for item in meaningful_words]\n",
    "\n",
    "# PorterStemmer: A rule-based stemming algorithm that removes suffixes to reduce words to their root form. It follows a set of predefined rules and does not require training.\n",
    "# LancasterStemmer: A more aggressive rule-based stemming algorithm that performs faster but may be harsher in its reductions compared to the Porter Stemmer.\n",
    "# WordNetLemmatizer: Uses the WordNet lexical database to reduce words to their lemma form (dictionary form). It requires knowledge of the word's part of speech (POS) for accuracy.\n",
    "\n",
    "print(\"Porter Stemmer:\", port_result)\n",
    "print(\"\\nLancaster Stemmer:\", lanc_result)\n",
    "print(\"\\nLemmatized:\", lemma_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4a22c-c791-408a-b9fe-68bdecd80461",
   "metadata": {},
   "source": [
    "# Q3. Regular Expressions and Text Splitting:   \n",
    "- Take the original text from Question 1.   \n",
    "- Use regular expressions to:     \n",
    "    - Extract all words with more than 5 letters.  \n",
    "    - Extract all numbers (if any exist in their text).   \n",
    "    - Extract all capitalized words.    \n",
    "- Use text splitting techniques to:  \n",
    "    - Split the text into words containing only alphabets (removing digits and special characters).   \n",
    "    - Extract words starting with a vowel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a734889-641d-414c-a0a8-7f4cf1689b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words > 5 letter length: ['fascination', 'deeply', 'within', 'quantum', 'mechanics', 'universe', 'governed', 'everyday', 'intuition', 'concept', 'superposition', 'particle', 'multiple', 'states', 'simultaneously', 'observed', 'constantly', 'sparks', 'curiosity', 'fundamental', 'nature', 'reality', 'Entanglement', 'seemingly', 'spooky', 'connection', 'between', 'particles', 'allows', 'instantaneously', 'influence', 'regardless', 'distance', 'deeper', 'interconnectedness', 'beginning', 'Exploring', 'mathematical', 'elegance', 'underpins', 'phenomena', 'Schrödinger', 'equation', 'quantum', 'theory', 'deciphering', 'language', 'cosmos', 'largely', 'theoretical', 'aspects', 'potential', 'applications', 'quantum', 'computing', 'quantum', 'cryptography', 'promise', 'revolutionize', 'technology', 'making', 'endlessly', 'captivating', 'frontier', 'scientific', 'inquiry']\n",
      "\n",
      "Numbers: []\n",
      "\n",
      "Capitalized words: ['My', 'The', 'Entanglement', 'Exploring', 'While']\n",
      "\n",
      "Alphabet-only words: ['My', 'fascination', 'lies', 'deeply', 'within', 'the', 'realm', 'of', 'quantum', 'mechanics', 'a', 'universe', 'governed', 'by', 'rules', 'that', 'often', 'defy', 'our', 'everyday', 'intuition', 'The', 'concept', 'of', 'superposition', 'where', 'a', 'particle', 'can', 'exist', 'in', 'multiple', 'states', 'simultaneously', 'until', 'observed', 'constantly', 'sparks', 'my', 'curiosity', 'about', 'the', 'fundamental', 'nature', 'of', 'reality', 'Entanglement', 'the', 'seemingly', 'spooky', 'connection', 'between', 'particles', 'that', 'allows', 'them', 'to', 'instantaneously', 'influence', 'each', 'other', 'regardless', 'of', 'distance', 'hints', 'at', 'a', 'deeper', 'interconnectedness', 'we', 'are', 'only', 'beginning', 'to', 'grasp', 'Exploring', 'the', 'mathematical', 'elegance', 'that', 'underpins', 'these', 'phenomena', 'from', 'the', 'equation', 'to', 'quantum', 'field', 'theory', 'feels', 'like', 'deciphering', 'the', 'very', 'language', 'of', 'the', 'cosmos', 'While', 'still', 'largely', 'theoretical', 'in', 'many', 'aspects', 'the', 'potential', 'applications', 'of', 'quantum', 'computing', 'and', 'quantum', 'cryptography', 'promise', 'to', 'revolutionize', 'technology', 'as', 'we', 'know', 'it', 'making', 'this', 'field', 'an', 'endlessly', 'captivating', 'frontier', 'of', 'scientific', 'inquiry']\n",
      "\n",
      "Words starting with vowels: ['of', 'a', 'universe', 'often', 'our', 'everyday', 'intuition', 'of', 'a', 'exist', 'in', 'until', 'observed', 'about', 'of', 'Entanglement', 'allows', 'instantaneously', 'influence', 'each', 'other', 'of', 'at', 'a', 'interconnectedness', 'are', 'only', 'Exploring', 'elegance', 'underpins', 'equation', 'of', 'in', 'aspects', 'applications', 'of', 'and', 'as', 'it', 'an', 'endlessly', 'of', 'inquiry']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input string for analysis\n",
    "input_string = new_paragraph\n",
    "\n",
    "# Find words exceeding five characters\n",
    "long_strings = re.findall(r'\\b\\w{6,}\\b', input_string)\n",
    "\n",
    "# Extract any sequence of digits\n",
    "found_numbers = re.findall(r'\\d+', input_string)\n",
    "\n",
    "# Identify words starting with a capital letter followed by lowercase letters\n",
    "leading_capital_words = re.findall(r'\\b[A-Z][a-z]*\\b', input_string)\n",
    "\n",
    "# Select words containing only alphabetic characters\n",
    "pure_alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', input_string)\n",
    "\n",
    "# Filter words that commence with a vowel\n",
    "starting_vowel_words = [item for item in pure_alpha_words if item[0].lower() in 'aeiou']\n",
    "\n",
    "print(\"Words > 5 letter length:\", long_strings)\n",
    "print(\"\\nNumbers:\", found_numbers)\n",
    "print(\"\\nCapitalized words:\", leading_capital_words)\n",
    "print(\"\\nAlphabet-only words:\", pure_alpha_words)\n",
    "print(\"\\nWords starting with vowels:\", starting_vowel_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d165d-c759-453e-a8c6-8ca723496e46",
   "metadata": {},
   "source": [
    "# Q4. Custom Tokenization & Regex-based Text Cleaning:   \n",
    "- Take original text from Question 1   \n",
    "- Write a custom tokenization function that:   \n",
    "    - Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").   \n",
    "    - Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).   \n",
    "    - Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as it is).   \n",
    "- Use Regex Substitutions (re.sub) to:   \n",
    "    - Replace email addresses with `<EMAIL>` placeholder.   \n",
    "    - Replace URLs with `<URL>` placeholder.   \n",
    "    - Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with `<PHONE>` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0028bce9-92a4-4832-8374-742e520d792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokens: ['Contact', 'us', 'at', 'support', 'website', 'com', 'See', 'more', 'at', 'https', 'website', 'info', 'or', 'ring', 'us', 'at', '91', '7777777777']\n",
      "Cleaned Text: Contact us at <EMAIL> See more at <URL> or ring us at <PHONE>.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example string containing contact information\n",
    "example_string = \"Contact us at support@website.com. See more at https://website.info or ring us at +91 7777777777.\"\n",
    "\n",
    "# Specialized token extraction method\n",
    "def specialized_token_extraction(text):\n",
    "    # Preserve hyphenated and apostrophed words, as well as decimal and whole numbers\n",
    "    return re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\", text)\n",
    "\n",
    "# Generate tokens from the example string\n",
    "generated_tokens = specialized_token_extraction(example_string)\n",
    "\n",
    "# Replace identifiable patterns with placeholders\n",
    "modified_text = re.sub(r'\\S+@\\S+', '<EMAIL>', example_string)\n",
    "modified_text = re.sub(r'https?://\\S+', '<URL>', modified_text)\n",
    "modified_text = re.sub(r'\\+91\\s?\\d{10}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', modified_text)\n",
    "\n",
    "print(\"Custom Tokens:\", generated_tokens)\n",
    "print(\"Cleaned Text:\", modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c7602-3b33-4eae-851a-1667db4f5511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
